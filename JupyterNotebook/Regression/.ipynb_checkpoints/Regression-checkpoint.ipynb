{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.61731124   5.44229298]\n",
      " [  0.4469666    0.74651127]\n",
      " [ -3.05369619  -4.41306661]\n",
      " [  6.85336066  10.21644765]\n",
      " [  2.97286717   4.47826094]\n",
      " [  9.27060176  13.78770528]\n",
      " [  2.25811094   3.42140334]\n",
      " [ -7.13477033 -10.44970788]\n",
      " [ -1.13567212  -1.58090054]\n",
      " [ -4.7572576   -6.92150381]\n",
      " [ -9.13084524 -13.3921784 ]\n",
      " [ -5.28663554  -7.7071681 ]\n",
      " [ -8.36381767 -12.27126613]\n",
      " [ -8.51067749 -12.48356053]\n",
      " [  9.90437603  14.71592396]\n",
      " [  5.15896611   7.70926706]\n",
      " [ -2.77932503  -4.01915655]\n",
      " [  0.1082475    0.25421451]\n",
      " [  8.93338075  13.27321399]\n",
      " [  9.7564652   14.50931665]\n",
      " [ -1.93093733  -2.76250119]\n",
      " [  8.54642985  12.71080197]\n",
      " [ -3.19480533  -4.63557468]\n",
      " [  5.77920543   8.63080314]\n",
      " [  7.77236736  11.56485695]\n",
      " [ -4.98328218  -7.2712047 ]\n",
      " [ -0.52142056  -0.68950218]\n",
      " [  3.45735179   5.20381574]\n",
      " [ -9.31265421 -13.66748602]\n",
      " [  1.93228131   2.93255584]\n",
      " [  5.65602114   8.4433465 ]\n",
      " [  5.90531805   8.79133383]\n",
      " [ -4.71959775  -6.89278203]\n",
      " [ -1.47895967  -2.08922034]\n",
      " [ -7.49469881 -10.98450209]\n",
      " [ -2.16595106  -3.11656929]\n",
      " [  6.35026198   9.4604121 ]\n",
      " [  7.50750817  11.18601424]\n",
      " [ -4.64562554  -6.78859874]\n",
      " [ -4.13012852  -6.01667742]\n",
      " [ -8.23007121 -12.05870396]\n",
      " [  4.25441336   6.36584296]\n",
      " [ -9.34488789 -13.68432623]\n",
      " [ -0.83729775  -1.13652187]\n",
      " [  4.55311134   6.80742585]\n",
      " [  8.06783856  12.01378022]\n",
      " [ -1.63196082  -2.32493794]\n",
      " [  2.677063     4.03943109]\n",
      " [  9.62944427  14.31982651]\n",
      " [  6.10194542   9.09885955]\n",
      " [ -5.81183405  -8.50371844]\n",
      " [ -3.5546956   -5.15460366]\n",
      " [  3.44262771   5.17581682]\n",
      " [  4.959381     7.42456652]\n",
      " [  4.30055212   6.43450544]\n",
      " [ -0.16559993  -0.16173522]\n",
      " [ -2.46760242  -3.55821263]\n",
      " [ -3.80278868  -5.52582689]\n",
      " [  9.13540945  13.58660332]\n",
      " [ -2.94530463  -4.25638731]\n",
      " [  8.47837522  12.60808781]\n",
      " [  2.70854177   4.0906262 ]\n",
      " [  1.42009539   2.1918712 ]\n",
      " [  0.55410797   0.90421419]\n",
      " [ -4.7559297   -6.93395515]\n",
      " [  2.98052132   4.49740939]\n",
      " [  9.76424447  14.51255746]\n",
      " [ -9.89709863 -14.53788886]\n",
      " [ -7.07354371 -10.36065382]\n",
      " [ -4.72606799  -6.87947432]\n",
      " [ -0.67983134  -0.91054488]\n",
      " [  7.58649967  11.30453688]\n",
      " [  9.68367098  14.39273323]\n",
      " [ -1.87654687  -2.67931074]\n",
      " [  1.29823187   2.00444875]\n",
      " [  5.06831666   7.56628358]\n",
      " [  5.39562622   8.042468  ]\n",
      " [  5.62067919   8.38933342]\n",
      " [  1.34210731   2.05130904]\n",
      " [ -7.9230494  -11.61604149]\n",
      " [  1.05895232   1.64543202]\n",
      " [ -0.26983876  -0.29721208]\n",
      " [ -8.1031381  -11.88218381]\n",
      " [ -0.92953591  -1.28305682]\n",
      " [ -7.78335628 -11.41320042]\n",
      " [ -9.60902158 -14.09888432]\n",
      " [  0.67427458   1.07703893]\n",
      " [ -0.58112799  -0.78162873]\n",
      " [ -8.827734   -12.94743305]\n",
      " [  5.3801361    8.01882798]\n",
      " [  4.84220783   7.25037777]\n",
      " [ -1.05696062  -1.46449214]\n",
      " [ -5.11231699  -7.4834283 ]\n",
      " [ -7.16981017 -10.50172709]\n",
      " [ -5.17550078  -7.55424282]\n",
      " [  6.19648044   9.22675277]\n",
      " [  4.82333641   7.20879734]\n",
      " [  7.6440788   11.37127076]\n",
      " [ -5.63458215  -8.23124932]\n",
      " [ -7.53420082 -11.04640515]]\n",
      "iteration:0, loss:7.843514885639977, w:0.9950721461930918, b:0.008735683656069047\n",
      "iteration:50, loss:0.0008409005138141858, w:1.4771505177334614, b:0.06061252720936714\n",
      "iteration:100, loss:0.00017233017353159667, w:1.4770230705369993, b:0.07826686996414331\n",
      "iteration:150, loss:8.335601944216827e-05, w:1.4769765774195678, b:0.08470722686224745\n",
      "iteration:200, loss:7.151523079368695e-05, w:1.476959616591644, b:0.08705668826962465\n",
      "iteration:250, loss:6.99394442379974e-05, w:1.4769534292310251, b:0.08791377879924732\n",
      "iteration:300, loss:6.972973664648473e-05, w:1.4769511720634152, b:0.08822644798107013\n",
      "iteration:350, loss:6.970182850513208e-05, w:1.476950348641938, b:0.08834051063126042\n",
      "iteration:400, loss:6.969811445573543e-05, w:1.476950048255327, b:0.0883821210262875\n",
      "iteration:450, loss:6.969762018552436e-05, w:1.4769499386733953, b:0.08839730062251304\n",
      "iteration:500, loss:6.969755440744277e-05, w:1.4769498986975798, b:0.08840283818449533\n",
      "iteration:550, loss:6.969754565361683e-05, w:1.476949884114283, b:0.08840485830362449\n",
      "iteration:600, loss:6.969754448864436e-05, w:1.4769498787942528, b:0.08840559524910861\n",
      "iteration:650, loss:6.969754433361093e-05, w:1.4769498768534899, b:0.08840586408901949\n",
      "iteration:700, loss:6.969754431297868e-05, w:1.4769498761454938, b:0.08840596216262085\n",
      "iteration:750, loss:6.969754431023213e-05, w:1.4769498758872148, b:0.08840599794016318\n",
      "iteration:800, loss:6.969754430986542e-05, w:1.4769498757929937, b:0.08840601099191735\n",
      "iteration:850, loss:6.969754430981881e-05, w:1.4769498757586217, b:0.08840601575323619\n",
      "iteration:900, loss:6.969754430981115e-05, w:1.4769498757460826, b:0.08840601749017951\n",
      "iteration:950, loss:6.969754430981206e-05, w:1.4769498757415085, b:0.08840601812382166\n",
      "Final loss:6.969754430981024e-05, w:1.4769498757398591, b:0.08840601835227159\n"
     ]
    }
   ],
   "source": [
    "# 1 采样数据\n",
    "data = []# 保存样本集的列表\n",
    "for i in range(100): # 循环采样 100 个点\n",
    "    # numpy.random.uniform(low,high,size)\n",
    "    # 从一个均匀分布[low,high)中随机采样，默认返回一个值\n",
    "    x = np.random.uniform(-10., 10.) # 随机采样输入 x\n",
    "    # 采样高斯噪声，高斯分布\n",
    "    eps = np.random.normal(0., 0.01)\n",
    "    # 得到模型的输出\n",
    "    y = 1.477 * x +0.089 + eps\n",
    "    data.append([x, y])\n",
    "data = np.array(data)\n",
    "print(data)\n",
    "\n",
    "# 2 计算误差\n",
    "def mse(b, w, points):\n",
    "    # 根据当前的 w,b 参数计算均方差损失\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)): # 循环迭代所有点\n",
    "        x = points[i, 0] # 获得 i 号点的输入 x\n",
    "        y = points[i, 1] # 获得 i 号点的输出 y\n",
    "        # 计算差的平方，并累加\n",
    "        totalError += (y - (w * x + b)) ** 2\n",
    "    # 将累加的误差求平均，得到均方差\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "# 3 计算梯度\n",
    "def step_gradient(b_current, w_current, points, lr):\n",
    "    # 计算误差函数在所有点上的导数，并更新 w,b\n",
    "    b_gradient = 0\n",
    "    w_gradient = 0\n",
    "    M = float(len(points)) # 总样本数\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # 误差函数对 b 的导数： grad_b = 2(wx+b-y)，参考公式(2.3)\n",
    "        b_gradient += (2/M) * ((w_current * x + b_current) - y)\n",
    "        # 误差函数对 w 的导数： grad_w = 2(wx+b-y)*x，参考公式(2.2)\n",
    "        w_gradient += (2/M) * x * ((w_current * x + b_current) - y)\n",
    "        # 根据梯度下降算法更新 w',b',其中 lr 为学习率\n",
    "        new_b = b_current - (lr * b_gradient)\n",
    "        new_w = w_current - (lr * w_gradient)\n",
    "    return [new_b, new_w]\n",
    "\n",
    "# 4 更新梯度\n",
    "def gradient_descent(points, starting_b, starting_w, lr, num_iterations):\n",
    "    # 循环更新 w,b 多次\n",
    "    b = starting_b # b 的初始值\n",
    "    w = starting_w # w 的初始值\n",
    "    # 根据梯度下降算法更新多次\n",
    "    for step in range(num_iterations):\n",
    "        # 计算梯度并更新一次\n",
    "        b, w = step_gradient(b, w, np.array(points), lr)\n",
    "        loss = mse(b, w, points) # 计算当前的均方差，用于监控训练进度\n",
    "        if step%50 == 0: # 打印误差和实时的 w,b 值\n",
    "            print(f\"iteration:{step}, loss:{loss}, w:{w}, b:{b}\")\n",
    "    return [b, w] # 返回最后一次的 w,b\n",
    "\n",
    "def main():\n",
    "    # 加载训练集数据，这些数据是通过真实模型添加观测误差采样得到的\n",
    "    lr = 0.01  # 学习率\n",
    "    initial_b = 0  # 初始化 b 为 0\n",
    "    initial_w = 0  # 初始化 w 为 0\n",
    "    num_iterations = 1000\n",
    "    # 训练优化 1000 次，返回最优 w*,b*和训练 Loss 的下降过程\n",
    "    [b, w] = gradient_descent(data, initial_b, initial_w, lr, num_iterations)\n",
    "    loss = mse(b, w, data)  # 计算最优数值解 w,b 上的均方差\n",
    "    print(f'Final loss:{loss}, w:{w}, b:{b}')\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
